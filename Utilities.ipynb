{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61279a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBetaParams(mus,sigma2s):\n",
    "    '''\n",
    "    Converts the mean and variance to a and b.\n",
    "    Only valid if sigma2 <= mu(1-mu)\n",
    "    '''\n",
    "    assert np.all(sigma2s <= mus*(1-mus))\n",
    "    num = (sigma2s + mus**2 - mus)/sigma2s\n",
    "    a = -1*mus*num\n",
    "    b = num*(mus - 1)\n",
    "    assert np.all(a > 0) or np.all(b > 0), 'Consider lowering noise levels'\n",
    "    return a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf16517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addNoise(df,colname,output_colname,y_noise):\n",
    "    '''\n",
    "    Add noise to the deltas. Models the noisy output as a sample from\n",
    "    a Beta distribution. y_noise is the standard deviation around the mean df[colname]\n",
    "    '''\n",
    "    y = df[colname]\n",
    "    a,b = getBetaParams(y,y_noise**2)\n",
    "    y_mod = beta(a,b).rvs(len(y))\n",
    "    y_mod = np.round(y_mod,3)\n",
    "    df[output_colname] = y_mod\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda26cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimateCoeffs(df,alpha_cols):\n",
    "    x = df[alpha_cols]\n",
    "    y = df['y'].values\n",
    "    md = LinearRegression(fit_intercept=False)\n",
    "    md.fit(x,y)\n",
    "    # calculate error\n",
    "    yhat = md.predict(x)\n",
    "    sigma2 = np.sum((yhat - y)**2)/(df.shape[0]-x.shape[1])\n",
    "    # calculate variance of estimate\n",
    "    var = np.linalg.inv(np.dot(x.T,x))*sigma2\n",
    "    return md.coef_,var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5e7e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimateCoeffsBayesian(df,alpha_cols):\n",
    "    x = df[alpha_cols]\n",
    "    y = df['y'].values\n",
    "    md = LinearRegression(fit_intercept=False)\n",
    "    md.fit(x,y)\n",
    "    # calculate error\n",
    "    yhat = md.predict(x)\n",
    "    sigma2 = np.sum((yhat - y)**2)/(df.shape[0]-x.shape[1])\n",
    "    # calculate variance of estimate\n",
    "    var = np.linalg.inv(np.dot(x.T,x))*sigma2\n",
    "    return md.coef_,var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20e518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impSampFast(df,alphacols,qparams,pparams,qvar,pvar,T=10000):\n",
    "    # run importance sampling\n",
    "    # set up the q distribution\n",
    "    # the coef qvar helps it budge from the q distribution - the var from the least square estimate can help us guide if we \n",
    "    # need to budge far away from the information- if more, then we don't need to budge; if less, then budge\n",
    "    assert T > 1, 'T has to be greater than 1'\n",
    "    qdiric = dirichlet(qvar*qparams)\n",
    "    pdiric = dirichlet(pvar*pparams)\n",
    "    draws = qdiric.rvs(T)\n",
    "    logqvals = qdiric.logpdf(draws.T)\n",
    "    logpriors = pdiric.logpdf(draws.T)\n",
    "\n",
    "    x = df[alphacols].values\n",
    "    y = df['y'].values\n",
    "    xb_all = np.dot(x,draws.T)\n",
    "    mus_all = np.exp(xb_all)/(1 + np.exp(xb_all))\n",
    "    phis = np.repeat(df['y_var'].values[:,np.newaxis],T,axis=1)\n",
    "    a_all,b_all = getBetaParams(mus_all,phis)\n",
    "    logliks = beta(a_all,b_all).logpdf(y[:,np.newaxis]).sum(axis=0)\n",
    "    logpvals = logpriors + logliks\n",
    "    logwts = logpvals - logqvals\n",
    "    wts = np.exp(logwts)\n",
    "    \n",
    "    draws = np.array(draws).squeeze()\n",
    "    imp_samp_df = pd.DataFrame(np.vstack([np.array(logpriors),np.array(logliks),\n",
    "                                          np.array(logqvals),np.array(logpvals),np.array(wts)]).T,\n",
    "                           columns=['logprior','loglik','logqval','logpval','wt'])\n",
    "    imp_samp_df['wt_norm'] = imp_samp_df['wt']/imp_samp_df['wt'].sum()\n",
    "    \n",
    "    #imp_samp_df.dropna(inplace=True)\n",
    "    #assert imp_samp_df.shape[0] > 0, 'No samples were close to target distribution, consider reducing (phi) noise levels'\n",
    "    #  self-normalized importance sampling estimate\n",
    "    post_exp_value = np.sum(draws*wts[:,np.newaxis],axis=0)/np.sum(wts)\n",
    "\n",
    "    #  self-normalized importance variance estimate\n",
    "    wts_norm = wts/np.sum(wts)\n",
    "    post_var = np.sum((wts_norm[:,np.newaxis]**2)*(draws - post_exp_value)**2,axis=0)\n",
    "    \n",
    "    return post_exp_value,post_var,imp_samp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a13ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function calculates the pro support for a topic on sm via sensing\n",
    "def runSimFast(N,sm_dist,survey_x,ideol_dist,part_bias):\n",
    "    '''\n",
    "    N         - number of sm users for a given topic for sensing\n",
    "    sm_dist   - demographic breakup on that sm\n",
    "    survey_x  - response rates for demographic subgroups from surveys\n",
    "                broken-up by ideologies\n",
    "    ideol_dist- probability indicating the ideological distribution\n",
    "    part_bias - participation bias induced by ideology\n",
    "                (scalar value between -1 and +1)\n",
    "                \n",
    "    The ideologies are denoted as ideol0 and ideol1. A positive part_bias\n",
    "    indicates that ideol1 is more likely to participate and negative\n",
    "    part_bias indicates that ideol0 is more likely to participate.\n",
    "    '''\n",
    "    # precompute the participation probability for each ideology\n",
    "    \n",
    "    if part_bias > 0:\n",
    "        part_probs = {'ideol1':1, 'ideol0':1 - part_bias}\n",
    "        \n",
    "    if part_bias < 0:\n",
    "        part_probs = {'ideol1':1 + part_bias, 'ideol0':1}\n",
    "    \n",
    "    if part_bias == 0:\n",
    "        part_probs = {'ideol1':1 , 'ideol0':1}\n",
    "    # picking a particular demographic group for N users. \n",
    "    # dems is a matrix of N X d, and each row will have a single 1\n",
    "    dems = multinomial(1,np.array(list(sm_dist.values()))).rvs(N)\n",
    "    dem_names = {}\n",
    "    for i,k in enumerate(sm_dist.keys()):\n",
    "        dem_names[i] = k\n",
    "\n",
    "    # picking ideology for N users.\n",
    "    ideols = bernoulli(ideol_dist).rvs(N)\n",
    "    dem_inds = np.where(dems == 1)[1] + 1\n",
    "\n",
    "    ideols_df = pd.DataFrame(ideols,columns=['ideol'])\n",
    "    ideols_df['ideol'] = ideols_df['ideol'].astype(str)\n",
    "    ideols_df['ideol_st'] = 'ideol'+ideols_df['ideol']\n",
    "\n",
    "    part_probs_list = ideols_df['ideol_st'].map(part_probs).values\n",
    "    part_flags = bernoulli(part_probs_list).rvs(N)\n",
    "\n",
    "    ideols_df['dem'] = dem_inds\n",
    "    ideols_df['dem'] = ideols_df['dem'].astype(str)\n",
    "    ideols_df['dem_st'] = 'dem'+ideols_df['dem']\n",
    "\n",
    "    ideols_df['f_dem_st'] = 'x_' + ideols_df['dem_st'] + '_'+ideols_df['ideol_st']\n",
    "    ideols_df['part_flag'] = part_flags\n",
    "\n",
    "    \n",
    "    dem_resp_rate = ideols_df['f_dem_st'].map(survey_x)\n",
    "    resp = bernoulli(dem_resp_rate).rvs(N)\n",
    "    ideols_df['response'] = resp\n",
    "    responses_sm = resp[np.where(part_flags == 1)[0]]\n",
    "    y_dem=len(np.where(responses_sm == 1)[0])/len(responses_sm)\n",
    "    return y_dem, ideols_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b569f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEAN AVERAGE PERCENTAGE ERROR (MAPE)\n",
    "def normmae(actual,predicted,norm=True):\n",
    "    if norm:\n",
    "        return np.mean(np.abs((actual - predicted)/actual))\n",
    "    else:\n",
    "        return np.mean(np.abs((actual - predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58787bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simSM(df_survey_dem_x,num_users,sm_dem_dist,ideol_dist,var_prior,noise_level,\n",
    "          part_bias_diverse=True,part_bias=0):\n",
    "    dem_y = {}\n",
    "    true_dem_dist = {}\n",
    "    if part_bias_diverse:\n",
    "        part_bias_dist = beta(1,1)\n",
    "    part_biases = []\n",
    "    # iterates over surveys\n",
    "    for t in df_survey_dem_x.index:\n",
    "        if part_bias_diverse:\n",
    "            part_bias = 2*(0.5-part_bias_dist.rvs(1))[0]\n",
    "        part_biases.append(part_bias)\n",
    "        _dem_y_t,responses_sm = runSimFast(num_users,sm_dem_dist,\n",
    "                              df_survey_dem_x.loc[t].to_dict(),\n",
    "                              ideol_dist,part_bias)\n",
    "        dem_y[t] = _dem_y_t\n",
    "        _true_dem_dist = getTrueDemDist(responses_sm)\n",
    "        true_dem_dist[t] = _true_dem_dist\n",
    "        \n",
    "\n",
    "    # convert to a DataFrame\n",
    "    dem_y_dict = {'y_true':dem_y}\n",
    "    true_dem_dict = pd.DataFrame.from_dict(true_dem_dist,\n",
    "                                 orient='index')\n",
    "    true_dem_dict.columns = ['true_'+_c[2:] for _c in df_survey_dem_x.columns]\n",
    "    df = df_survey_dem_x.join(pd.DataFrame.from_dict(dem_y_dict)).join(true_dem_dict)\n",
    "    # add noise and sample variance\n",
    "    df = addNoise(df,'y_true','y',noise_level*np.ones((df.shape[0],)))\n",
    "    invg = invgamma(1/var_prior)\n",
    "    df['y_var'] = invg.rvs(df.shape[0])\n",
    "    return df,part_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085a2367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate true demographic distribution of participants\n",
    "def getTrueDemDist(responses_sm):\n",
    "    responses_sm_p = responses_sm.loc[responses_sm['part_flag'] == 1]\n",
    "    return (responses_sm_p.groupby('f_dem_st').size()/responses_sm_p.shape[0]).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ae8359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run inference experiments\n",
    "def runInfExptFixed(df,prior_sm_dem_dist,qvar,pvar,alphacols,pprior=[]):    \n",
    "    w,var = estimateCoeffs(df,alphacols)\n",
    "    wnorm = w.copy()\n",
    "    wnorm[np.where(wnorm <= 0)[0]] = 1e-4\n",
    "    if len(pprior) == 0:\n",
    "        pparams = wnorm\n",
    "    else:\n",
    "        pparams = pprior\n",
    "    qparams = prior_sm_dem_dist\n",
    "    post_w,post_var,imp_samp_df = impSampFast(df,\n",
    "                                          alphacols,\n",
    "                                          qparams,pparams,\n",
    "                                          qvar=qvar,pvar=pvar)\n",
    "    return w,var,post_w,post_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b248fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate errors\n",
    "def calcErrors(est_ws,est_vars,est_post_ws,est_post_vars,fact_sm_dem_dist):\n",
    "    est_errors = []\n",
    "    est_pes = []\n",
    "    for w,var in zip(est_ws,est_vars):\n",
    "        est_errors.append(np.abs(w - fact_sm_dem_dist))\n",
    "        est_pes.append(np.abs((w - fact_sm_dem_dist)/fact_sm_dem_dist))\n",
    "    est_errors = np.array(est_errors)\n",
    "    est_pes = np.array(est_pes)\n",
    "\n",
    "    est_post_errors = []\n",
    "    est_post_pes = []\n",
    "    for w,var in zip(est_post_ws,est_post_vars):\n",
    "        est_post_errors.append(np.abs(w - fact_sm_dem_dist))\n",
    "        est_post_pes.append(np.abs((w - fact_sm_dem_dist)/fact_sm_dem_dist))\n",
    "    est_post_errors = np.array(est_post_errors)\n",
    "    est_post_pes = np.array(est_post_pes)\n",
    "\n",
    "    res = {}\n",
    "    res['truth'] = fact_sm_dem_dist\n",
    "    res['ls_mean'] = np.array(est_ws).mean(axis=0)\n",
    "    res['ls_mean_std'] = np.std(np.array(est_ws),axis=0)\n",
    "    res['ls_var'] = np.array(est_vars).mean(axis=0)\n",
    "    res['ls_errors'] = np.array(est_errors).mean(axis=0)\n",
    "    res['ls_pes'] = np.array(est_pes).mean(axis=0)\n",
    "\n",
    "    res['is_mean'] = np.array(est_post_ws).mean(axis=0)\n",
    "    res['is_mean_std'] = np.std(np.array(est_post_ws),axis=0)\n",
    "    res['is_var'] = np.array(est_post_vars).mean(axis=0)\n",
    "    res['is_errors'] = np.array(est_post_errors).mean(axis=0)\n",
    "    res['is_pes'] = np.array(est_post_pes).mean(axis=0)\n",
    "    \n",
    "    return res\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
